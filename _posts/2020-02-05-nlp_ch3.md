---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch3.전통적인 언어 모델)"
subtitle:   "NLP"
categories: pro
tags: nlp
comments: true
---
  
언어모델 중의 통계에 기반한 전통적인 언어 모델에 대해 배워보자
  
---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/21695) 에 대한 글을 요약하였다.

# Ch03. 언어 모델 (Language Model)
- 이번 챕터에서는 전통적인 통계 기반 언어 모델(Statistical Language Model, SLM)에 대해 배운다.
- 요즘은 신기술인 GPT나 BERT 또한 인공신경망 언어 모델이나 나중 챕터에서 공부하자.

## 1) 언어 모델(Language Model)이란?
- 언어라는 현상을 모델링 하고자 단어 시퀀스(or 문장)에 확률을 할당(assign)하는 모델
- 이전 단어들이 주어졌을때 다음 단어를 예측하는 것 (※BERT는 양쪽 단어들로부터 가운데 단어를 예측하는 모델)
- 주어진 이전 단어로부터 다음 단어 예측하기 : 조건부확률 이용

## 2) 통계적 언어 모델(Statistical Language Model, SLM)
- 카운트 기반 : P(is|An adorable little boy)=count(An adorable little boy is) / count(An adorable little boy)
- 카운트 기반 접근 한계 : 
    - 분자가 0이면 확률 0, 분모가 0이면 확률 정의되지않음
    - 데이터가 방대하지 않은 경우 저런 문장이 없어서 희소 문제(sparsity problem)이 생긴다.

## 3) N-gram 언어 모델(N-gram Language Model)
- 확률을 계산하고 싶은 문장이 길어질수록 코퍼스에서 그 문장이 존재하지않을 가능성 높음
- 단어들을 줄이면 카운트를 할수 있음
- N-gram : N개의 연속적인 단어 나열
- N-gram Language Model의 한계 :
    - 희소 문제
    - n을 선택하는 것은 trade-off 문제 : n이 커질수록 모델사이즈가 커진다. 희소성이 심각해진다. n이 작아지면 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도가 떨어진다.

## 4) 한국어에서의 언어 모델
- 한국어는 어순 중요하지않다
- 한국어는 교착어이다
- 한국어는 띄어쓰기가 제대로 지켜지지 않는다.

## 5) 펄플레서티(Perplexity)
- 모델 A,B 모델 성능 비교하려면?
    - 일일히 모델을 실제 작업하는 거는 공수가 많이 듦(외부평가)
    - Perplexity : 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 게 펄플레서티이다(내부평가)
- Perplexity(PPL) : 당혹이라는 뜻으로 헷갈리는 정도라고 보면 되겠다(마치 엔트로피랑 비슷)
※ PPL= exp(Cross Entropy) [참고자료](https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-8/03-perpexity)
- PPL이 낮을수록 성능이 좋다. (=헷갈리는게 낮을수록 좋다.)
- 분기계수라고도 하는 예를들어 주사위 나오는 수가 1/6이니까 분기계수는 6


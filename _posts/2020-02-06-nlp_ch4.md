---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch4.카운트 기반의 단어표현)"
subtitle:   "NLP"
categories: pro
tags: nlp
comments: true
---
  
이번에는 문자를 카운트 기반의 숫자로 수치화 하는 법을 배우자.  
---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/24557) 에 대한 글을 요약하였다. 

# Ch04. 카운트 기반의 단어 표현 (Count based word Representation))
- 텍스트를 표현하는 방법은 여러가지이다.
- 앞에서 배운 n-gram 또한 텍스트를 표현하는 방법이다.(혹시 기억 못 할까봐.. N-gram은 카운트기반으로 갯수 정해서 다음 단어가 나올 확률이었다.)
- (주제) 이 챕터에서는 모델을 만드는건 아니고 문자를 숫자로 수치화 하는 방법에 대해 말해본다.
  
## 1) 다양한 단어의 표현 방법
- 단어의 표현 방법 : 2가지로 나뉜다. 기억하자. 국소/분산 표현
    - 국소 표현(Local Representation) : 단어 자체
    : 예를들어, poppy, cute, lovely라는 단어가 있으면 1, 2, 3 인덱스 잡아서 매핑해버리는거
    - 분산 표현(Distributed Representation) : 주변 참고 단어 표현
    : 같은 예시에서 poppy라는 단어 근처에 주로 cute, lovely가 있으니까 poppy는 cute, lovely한 느낌이다 라는 뉘앙스를 표현하는거임

- 단어 표현의 카테고리화
<img src="https://wikidocs.net/images/page/31767/wordrepresentation.PNG" width="100%">  
이번 챕터에서는 국소표현의 카운트 베이스의 BoW와 그의 확장인 DTM과 단어의 중요도에 따른 가중치인 TF-IDF를 학습한다. 나머지는 천천히 공부해보자. 이전에 공부했던 0,1로 원핫인코딩과 N-gram도 보인다. 
  
## 2) Bag of Words(BoW)
- 순서 고려 하지 않고 단어들의 출현 빈도만 가지고 단어 인덱스에 대한 빈도 벡터를 만든것
- 불용어가 너무 자주 나오니까 사용자가 지우든, CounterVectorizer로 지우든 NLTK로 지우든 해야한다.
  
## 3) 문서 단어 행렬(Document-Term Matrix, DTM)
- 문서축과 단어 축으로 행렬 만든거
- 한계점 : 희소 행렬인거, 단순 빈도수 기반이라 불용어가 높은 수치를 차지하거나 그런 불필요한 단어들이 많이 나오면 가중치로 낮춰버려야 한다. 중요한건 가중치를 올리고. 그래서 다음에 TF-IDF를 배운다.
  
## 4) TF-IDF(Term Frequency-Inverse Document Frequency)
- TF(d,t) : 특정문서 d에 t단어가 있는 수
- IDF(t) : 전체문서중에 t단어가 있는 문서 수 의 역수를 로그 취함
- TF는 높아야 중요한거 맞고, IDF는 낮아야 좋은거니까(다른 문서에 없는데 특정 문서에만 있는 단어가 좋으니까) IDF(t) 만들 때 역수 로그로 취해서 t단어가 나온 문서가 적을수록 값이 높게 나오게 만듦
- 그래서 TF X IDF 해서 IDF가 가중치의 역할을 하는것

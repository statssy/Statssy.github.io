---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch6.토픽 모델링)"
subtitle:   "NLP"
categories: pro
tags: nlp
comments: true
---
  
(IMO)문서 집합의 토픽(주제)가 비슷한거 묶는 일종의 비지도학습이라고 보면 될 것 같다.
---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/24602) 에 대한 글을 요약하였다. 

# Ch06. 토픽 모델링(Topic Modeling)

## (그냥 이것부터 읽고 시작하기) LSA와 LDA의 차이
(IMO) 일단 A부터가 다르다. Analysis랑 Allocation..haha
- LSA : DTM을 차원 축소해서 토픽을 묶는다
- LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합 확률로 추정하여 토픽을 추출

## 1) 잠재 의미 분석(Latent Semantic Analysis, LSA)
- LDA는 LSA 단점 개선해서 태어난 알고리즘
- DTM이나 TF-IDF는 빈도수 기반이라 단어의 토픽을 고려하지 못한다는 단점이 있다.
- 선형대수학의 특이값 분해(Singular Value Decomposition, SVD)로 LSA를 할 수 있다.

### 1. 특이값 분해
- A = U X S X Vt 의 곱이다.
- U는 행의수가 문서수에 연관, 관련 Vt는 열의수가 단어수에 관련되어있다.
- U의 행의 수말고 열의수를 줄이면 결국 토픽의 수를 줄일 수 있다. 당연히 식을 세울라면 Vt는 값이 따라오겠지?
- 다시 말하면 S가 결국 U랑 Vt 곱하기 하려면 행렬수는 토픽수에 의해 정해 질것이고 당연히 대각행렬(대각선 빼고 0인 행렬, MxN에 따라 뒤에 0이 뒤에 다다닥 붙거나 아래로 0이 다다닥 붙거나 할 수 있지)
(IMO)차원 축소랑 똑같은 개념인  같다.
- 순서 : 전처리 -> TF-IDF 행렬 만들기 -> 토픽 수 정해서 토픽 모델링
- 결과 : 토픽1에는 단어A가 p% 영향을 줬고 단어 B는... 이런식으로 나온다.
- 장점 : 쉽고 빠르다
- 단점 : 추가 데이터 들어오면 처음부터 다시 계산

## 2) 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)
- 문서들은 토픽들의 혼합으로 구성되어 있고, 토픽들은 확률분포에 기반하여 단어들을 생성한다고 가정
- 데이터가 주어지면 LDA는 문서가 생성되던 과정을 역추적한다.
(IMO) 1번 문서는 토픽1일 확률 30%. 토픽2일 확률 20% 뭐 이런식으로 나온다. 어떻게 역추적하는지 알아보자
- 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합 확률로 추정하여 토픽을 추출

### 1. LDA의 수행 순서
- 토픽 개수 k개 알려주기
- 모든 단어에 랜덤하게 k개 중 하나의 토픽 할당
- 모든 문서의 모든 단어에 아래 사항 반복(iterative)
    - 만약에 특정 문서의 apple이라는 단어의 토픽을 알고 싶다.
    - 그러면 그 특정 문서에 토픽 중 많은거 선택
    - 문서 전체 기준으로 많은거 선택
- 다 토픽 정했으면 단어마다 토픽 존재 확률 구해 질꺼고 문서에도 특정 토픽이 존재하는 확률이 나올테니까 그 두개를 결합 확률하는 거
- 이렇게 계속 반복

### 2. LDA 실습 순서
- 단어 집합 만들기
- 모델 훈련 시키기 (토픽 마다 단어의 점수? 확률?이 나옴)
- LDA 시각화 (토픽들을 2차원에 두고, 가까우면 비슷한거고, 어떤 단어가 점수가 몇나오는지 나온다. 멋있다 솔직히)
<img src="https://wikidocs.net/images/page/30708/visualization_final.PNG" width="70%">
  
- 문서별 토픽 분포 보기 (ex. 문서A는 토픽 비중 (토픽7 : 30%, 토픽3 : 10% ...))

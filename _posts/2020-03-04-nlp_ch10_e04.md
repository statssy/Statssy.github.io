---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch10. 엘모 ELMo)"
subtitle:   "NLP"
categories: pro
tags: nlp
comments: true
---

워드 임베딩에서 문맥을 반영한 워드 임베딩이자, 사전 훈련된 언어 모델인 엘모에 대해 공부해보자

---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/33930) 에 대한 글을 요약하였다. 




# 10. 워드 임베딩(Word Embedding)의 종류

## 7) 엘모(Embeddings from Language Model, ELMo)
: 가장 큰 특징은 **사전 훈련된 언어 모델(Pre-trained language model)**을 사용

<img src="https://wikidocs.net/images/page/33930/elmo_DSHQjZD.png" width="30%"> 

---
### 1. ELMo(Embeddings from Language Model)
- 예를들어, Bank라는 단어는 Bank Account(은행 계좌)와 River Bank(강둑)에서의 전혀 다른 의미를 가진다.
- 문맥을 반영한 워드 임베딩(Contextualized Word Embedding)이 탄생

---
### 2. BiLM(Bidirectional Language Model)의 사전 훈련
  
<img src="https://wikidocs.net/images/page/33930/deepbilm.PNG" width="40%"> 

- 순방향(RNN 언어모델)만 일단 본다면 문장으로 부터 단어 단위로 입력을 받는데 RNN 내부의 은닉 상태 $$h_{t}$$는 시점(time-step)이 지날 수록 업데이트 됨
- 이는 $$h_{t}$$의 값이 문장의 문맥 정보를 점차적으로 반영한다고 볼 수 있음
- 순방향 뿐만아니라 반대 방향으로 문장을 스캔하는 역방향 RNN 또한 활용
- 순방향과 역방향 둘 다 활용하여 이 언어 모델을 BiLM(Bidirectional Language Model)이라고 함

<img src="https://wikidocs.net/images/page/33930/forwardbackwordlm2.PNG" width="60%"> 

- BiLM의 특징으로는 char CNN이라는 방법으로 글자 단위로 계산하여 마치 서브단어(subword)의 정보를 참고하여 예를 들어 dog와 doggy란 단어의 연관성을 찾을 수 있고 이 방법은 OOV에도 견고하다는 장점 있음

- 양방향 RNN VS ELMo의 BiLM
    - 양방향 RNN은 은닉 상태를 다음 층의 입력으로 보내기 전에 연결 시키지만 (=은닉이 이어짐)
    - ELMo의 BiLM은 순방향 언어 모델과 역방향 언어 모델을 각각의 은닉 상태만을 다음 은닉층에 보내며 훈련 시킨 후에 은닉상태를 연결(=따로 훈련 후 은닉 이음)

---
### 3. 1) biLM의 활용 
<img src="https://wikidocs.net/images/page/33930/playwordvector.PNG" width="60%"> 

- 이 예제에서는 play란 단어가 임베딩이 되고 있다는 가정 하에 ELMo를 설명
- play라는 단어를 임베딩 하기위해서 ELMo는 위의 점선의 사각형 내부의 각 층의 결과값을 재료로 사용. 다시 말해 각 층의 출력값을 가져옴
- 여기서 출력값이란 첫번째는 임베딩 층을 말하고, 나머지 층은 각 층의 은닉 상태를 말함.
- ELMo의 직관적인 아이디어 : 각 층의 출력값이 가진 정보는 전부 서로 다른 종류의 정보를 가지고 있고 이를 모두 활용한다는 것

### 3. 2) 임베딩 과정

1. 각층의 출력값을 연결(concatenate)

<img src="https://wikidocs.net/images/page/33930/concatenate.PNG" width="30%"> 
  
  
2. 각 층의 출력값 별로 가중치를 준다.

<img src="https://wikidocs.net/images/page/33930/weight.PNG" width="30%"> 
  
  
3. 각 층의 출력값을 모두 더한다. (가중합을 구한다.)

<img src="https://wikidocs.net/images/page/33930/weightedsum.PNG" width="50%"> 
  
  
4. 벡터의 크기를 결정하는 스칼라 매개변수를 곱한다.

<img src="https://wikidocs.net/images/page/33930/scalarparameter.PNG" width="50%"> 
  
  

5. 이렇게 완성된 벡터를 ELMo 표현(represeentation)이라고 함. 지금까지 표현을 얻기 위한 방법이였고 이제 수행하고 싶은 자연어 처리 작업(텍스트 분류, 질의 응답 등등)을 정하자
  
  
6. 예를 들어 텍스트 분류 작업을 하기위해 GloVe와 같은 방법론을 사용한 임베딩 벡터가 있다면 ELMo표현을 GloVe 임베딩 벡터와 연결(concatenate)해서 입력을 사용
  
  
7. 위에서 사용한 $$s_{1}$$, $$s_{1}$$, $$s_{1}$$, $$γ$$는 훈련 과정에서 학습하고, ELMo 표현을 만드는데 사용되는 사전 훈련된 언어 모델의 가중치는 고정

<img src="https://wikidocs.net/images/page/33930/elmorepresentation.PNG" width="40%"> 



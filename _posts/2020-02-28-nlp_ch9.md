---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch09 .순환 신경망)"
subtitle:   "NLP"
categories: pro
tags: nlp
comments: true
---

앞에서 배운 피드 포워드 신경망은 입력의 길이가 고정 되어 있는 한계가 있는데 RNN은 시점 개념을 사용하여 이것을 해결 하게 됨. 최대한 간단하게 정리해보았다. 

---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/48558) 에 대한 글을 요약하였다. 

# Ch09 .순환 신경망(Recurrent Neural Network)

### 순환 신경망(Recurrent Neural Network, RNN)
- 앞서 배운 피드 포워드 신경망 같은 경우는 오직 신경망들이 출력층 방향으로만 향함
- 순환 신경망은 그렇지 않음
<img src="https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG" width="40%"> 

- 위의 그림의 초록색이 셀 (메모리셀, RNN셀이라고 표현)
- 현재 시점 t에서의 메모리 셀이 가지고 있는 값은 과거의 메모리 셀의 값에 영향을 받는다.
- 은닉상태 : t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값 (쉽게말해 이전 메모리셀이 가진 값)

<img src="https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG" width="40%"> 

- 일대다(one-to-many) 예시 : 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝(Image Captioning)
- 다대일(many-to-one) 예시 : 긍정인지 부정인지 판별 분류 / 스팸 메일 분류
- 다대다(many-to-many) 예시 : 입력문장으로 대답문장 출력하는 챗봇 / 번역기 / 개체명 인식, 품사 태깅


### 장단기 메모리(Long Short-Team Memory, LSTM)
- RNN 한계 : 비교적 짧은 시퀀스에 대해서만 효과를 보인다. 앞쪽 시점도 중요할지도 모른다.
ex) ''모스크바에 여행을 왔는데 건물도 예쁘고 먹을 것도 맛있었어. 그런데 글쎄 직장 상사한테 전화가 왔어. 어디냐고 묻더라구 그래서 나는 말했지. 저 여행왔는데요. 여기 ___'' 다음 단어를 예측

<img src="https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG" width="40%"> 

- 메모리 셀에 입력 게이트, 삭제 게이트, 출력게이트를 추가하여 불필요한 기억을 지우고 기억해야할 것을 정함


### 게이트 순환 유닛(Gated Recurrent Unit, GRU)
- LSTM은 입력, 삭제, 출력 이 있다면 GRU는 업데이트 게이트와 리셋 게이트 2가지만 존재
<img src="https://wikidocs.net/images/page/22889/GRU.PNG" width="40%"> 


### RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)

- RNNLM 장점 : N-gram이나 NNLM은 고정된 개수의 단어만 입력 받아야 했지만, 시점(time stamp) 개념이 도입된 RNN은 입력 길이 고정할 필요 없음

<img src="https://wikidocs.net/images/page/46496/rnnlm1_final_final.PNG" width="40%">

- 교사 강요(teacher forcing) : 위의 그림을 보면 예측 과정에서 이전 시점의 출력을 현재 시점의 입력(있는 정답)으로 한다. 진짜 출력을 다음 시점의 입력으로 하면 오예측으로 끝까지 오예측할 가능성 있음

<img src="https://wikidocs.net/images/page/46496/rnnlm4_final.PNG" width="40%">

- 위의 그림을 보면 입력 값을 임베이딩(단어간 유사도 반영하기 위함) -> Cell -> 출력 -> Cross Entoropy로 손실함수 사용해서 역전파 이루어지면서 가중치 행렬들 학습(당연 임베딩 벡터값들도 학습)

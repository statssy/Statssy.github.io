---
layout: post
title:  "[파이썬] 파이썬으로 배우는 응용텍스트 분석 (Ch3. 텍스트 벡터와 변환 파이프라인)"
subtitle:   "PYTHON"
categories: pro
tags: python
comments: true
---


파이썬으로 배우는 응용텍스트 분석 (Ch3. 텍스트 벡터와 변환 파이프라인) (P59 ~ P87까지 ) 

<img src="http://image.yes24.com/momo/TopCate2739/MidCate008/273872383.jpg" width="30%">  

---

# CH4. 텍스트 벡터와 변환 파이프라인

## 이번 장 초록
   
### 내용 측면
- 텍스트로 된 문서를 벡터화해야 한다.(=특징추출)
- 텍스트는 길이가 각각 다르다 vs 벡터는 항상 길이가 일정(승!!)
- 의미 공간(semantic space)는 유사한 의미를 가진 문서는 가깝고, 다른 문서는 멀게끔 사상(mapping)된다.
- 유사도를 구함으로써 문서의 주요 성분을 추출하고 결정 경계(decision boundaries)를 도출
- 가장 간단한 방식의 의미 공간은 단어 주머니(bag-of-words) 모델
   
### 코드 측면
- 이번 장에서는 NLTK의 언어 기술과 사이킷런 및 Gensim의 머신러닝 기술을 결합해 반복 재사용 할 수 있는 파이프라인에서 변환기를 만드는 방법을 보여줌
   

## 공간 내 단어
- **(중요)**인코딩의 4가지 유형 : 빈도, 원핫(one-hot), TF-IDF, 분산 표현
- NLTK, 사이킷런, Gensim로 구현(애플리케이션의 요구 사항에 맞게 골라야함)
- NLTK는 텍스트 데이터에 적합한 많은 메서드가 있지만 / 큰 의존성이 있다.(모듈간의 연결, 하나의 모듈이 바뀌면 다른 모듈도 변경되서 위험)
- 사이킷런은 텍스트를 염두하고 만들어진게 아니라 유용한 편의 기능 많음
- Gensim은 딕셔너리와 레퍼런스를 행렬 마켓 포맷(행렬형식을 주고 받을수 있음)으로 직렬화할 수 있으므로 여러 플랫폼에 유연


## 빈도 벡터
- **(중요)**가장 간단한 벡터 인코딩 모델은 문서에 나타나는 단어의 빈도를 벡터화

## 원핫 인코딩
- 빈도 기반 인코딩은 단어의 상대적 위치를 무시해서 지프 분포(=긴꼬리)로 인해 문제가 생김. [지프 분포 설명 참고](https://statkclee.github.io/text/nlp-zipf-law.html)
(불용어가 빈도가 많은데 사실 중요한건 중간 빈도)
- **(중요)**쉽게 말해 문서에 단어 존재 여부를 0, 1로 함. 그럼으로써 토큰(단어 단위 등)의 불균형 문제를 해결함
- 반복요소가 작은 문서에 효과적이며 평활 성질을 갖는 모델에 적용
- 일반적으로 인공 신경망에서 사용되며, 활성 함수는 [0, 1] 또는 [-1, 1]의 범위에 있어야한다.
- 모든 단어가 등거리로 측정되기 때문에 단어별 유사성을 인코딩할수 없다.(먼 건 그냥 다 멀 것이다.)

## 용어빈도-역문서빈도(TF-IDF, term frequency-inverse document frequency)
- 해당 토큰(단어)이 문서에 몇번 노출되는가? 그리고 전체 데이터 중 해당 단어는 얼마나 흔한 것인가?
ex) 야구 문서에서 '심판', '베이스, '덕아웃'같은 토큰이 자주 표시되는 반면, 다른 전체 문서에서는 많이 등장하는 '뛴다', '점수' '플레이'는 덜 중요하다.
- [TF-IDF 계산법 참고](https://sangmi820.tistory.com/entry/CHAPTER-3-%EA%B7%80%EB%82%A9-%ED%95%99%EC%8A%B5-tfidf-%EB%B0%A9%EB%B2%95)을 한다.
    - TF : 해당 문서에서의 단어 출현 횟수/해당 문서 모든 단어 횟수
    - IDF : log(단어가 나오는 문서 수/총 문서 수)
    - TFIDF = TF X IDF
- 불용어들 문제를 자연스럽게 해결함으로써 가중치를 적게 하면 된다. 그럼으로써 중요한 희소한 단어를 편향시킨다.

## 분산 표현
- 이 전에 나온 빈도, 원핫, TF-IDF 인코딩을 사용하면 서로 공유하지 않은 문서끼리는 비교할 수 없다.

### (추가 내용)분산 표현 내용 
 [분산표현 참고](https://wikidocs.net/22660)
- 그러므로 분포 가설이라는 가정 하에 만들어진다. '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'
- [0 0 0 0 1 0 0 0 0] -> [0.1 0.2 0.4 0.6 0.8 0.6 0.4 0.2 0.1] 이렇게 분산 형태로 표현

### (추가 내용)Word2Vec 내용
- Tomas Mikolov의 구글 연구원팀이 만든 word2vec은 분산 표현 모델이다.
- CBOW(Continuous Bag of Words)와 Skip-Gram 방식이 있다.

### CBOW(Continuous Bag of Words) VS Skip-Gram
- 주변에 있는 단어들을 가지고 중간에 있는 단어들을 예측하는 방법
- 반대로, Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법
- 쉽게 말해 주변 단어들 벡터에 값을 주는 방법


## 텍스트 벡터화 방법 개요
|   벡터화 방법  |           함수           |     용도    |                      고려할점                     |   |
|:--------------:|:------------------------:|:-----------:|:-------------------------------------------------:|:-:|
|      빈도      |      용어빈도를 센다     | 베이즈 모델 |    빈도가 높은 단어는 대부분 정보성이 높지 않다   |   |
| 원핫 인코딩 | 용어 출현을 이진화(0, 1) |    신경망   | 모든 단어들이 등거리이므로 추가 정규화가 중요하다 |   |
|                |                          |             |                                                   |   |




---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch10. 글로브 Glove)"
subtitle:   "NLP"
categories: pro
tags: nlp
comments: true
---

워드 임베딩에서 핫한 글로브(Glove)에 대한 이론을 공부해보자

---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22885) 에 대한 글을 요약하였다. 



# 10. 워드 임베딩(Word Embedding)의 종류

## 5) 글로브(Glove)
  
- 글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법론으로 2014년에 미국 스탠포드 대학에서 개발한 단어 임베딩 방법론이다.
- 기존의 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완하는 목적으로 나옴. 실제로 성능이 좋음
- Word2Vec와 Glove 중에 무엇이 뛰어나다고는 말할 수는 없고 두 가지를 비교해봐야함
  

### 1. 기존 방법론에 대한 비판
  
- 전에 배운 LSA와 Word2Vec 복습
  - LSA는 DTM이나 TF-IDF 행렬과 같이 각 문서에서의 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 받아 차원 축소하여 잠재된 의미를 끌어내는 방법론
  - Word2Vec는 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론

- LSA와 Word2Vec 장단점 그리고 Glove
  - LSA는 카운트 기반으로 전체적인 통계 정보를 고려하지만, 왕:남자=여왕:? 와 같은 단어 의미의 유추 작업(Analogy Task)에는 성능이 떨어집니다.
  - Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못함
  - Glove는 이런 각각의 한계를 지적하며 카운트 기반과 예측 기반 두가지 모두를 사용


### 2. 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)
: 단어의 동시 등장 행렬은 행과 열에 전체 단어 구성
  
e.g) 윈도우 크기는 1  
I like deep learning  
I like NLP  
I enjoy flying  

  
| 카운트   | I | like | enjoy | deep | learning | NLP | flying |
|----------|---|------|-------|------|----------|-----|--------|
| I        | 0 | 2    | 1     | 0    | 0        | 0   | 0      |
| like     | 2 | 0    | 0     | 1    | 0        | 1   | 0      |
| enjoy    | 1 | 0    | 0     | 0    | 0        | 0   | 1      |
| deep     | 0 | 1    | 0     | 0    | 1        | 0   | 0      |
| learning | 0 | 0    | 0     | 1    | 0        | 0   | 0      |
| NLP      | 0 | 1    | 0     | 0    | 0        | 0   | 0      |
| flying   | 0 | 0    | 1     | 0    | 0        | 0   | 0      |
  
  
  
### 3. 동시 등장 확률(Co-occurrence Probability)
: 동시 등장 확률 $$P(k\ |\ i)$$는 동시 등장 행렬로 부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률
  
e.g)  
$$P(k\ |\ i)$$에서 i를 중심 단어(Center Word), k를 주변 단어(Context Word)라고 했을 때, 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값이라고 볼 수 있음
  
| 동시 등장 확률과 크기 관계 비(ratio) | k=solid  | k=gas    | k=water | k=fasion |
|--------------------------------------|----------|----------|---------|----------|
| P(k l ice)                           | 0.00019  | 0.000066 | 0.003   | 0.000017 |
| P(k l steam)                         | 0.000022 | 0.00078  | 0.0022  | 0.000018 |
| P(k l ice) / P(k l steam)            | 8.9      | 0.085    | 1.36    | 0.96     |

- 위의 표 해석
: ice가 등장했을 때 solid가 등장할 확률 0.00019은 steam이 등장했을 때 solid가 등장할 확률인 0.000022보다 약 8.9배 크다

### 4. 손실 함수(Loss function)
- 용어 정리
  - $$X$$ : 동시 등장 행렬(Co-occurrence Matrix)
  - $$X_{ij}$$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수
  - $$X_{i} : \sum_j X_{ij}$$ : 동시 등장 행렬에서 i행의 값을 모두 더한 값
  - $$P_{ik}$$ : $$P(k\ |\ i)$$ = $$\frac{X_{ik}}{X_{i}}$$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률
    Ex) P(solid l ice) = 단어 ice가 등장했을 때 단어 solid가 등장할 확률
  - $$\frac{P_{ik}}{P_{jk}}$$ : $${P_{ik}}$$를 $${P_{jk}}$$로 나눠준 값
  - Ex) P(solid l ice) / P(solid l steam) = 8.9
  - $$w_{i}$$ : 중심 단어 i의 임베딩 벡터
  - $$\tilde{w_{k}}$$ : 주변 단어 k의 임베딩 벡터

- GloVe의 아이디어 한줄 요약 : '임베딩 된 중심 단어와 주변 단어 벡터의 내적이 코퍼스에서의 동시 등장 확률이 되도록 만드는것'
- 식 표현 : $$dot\ product(w_{i}\ \tilde{w_{k}}) \approx\ P(k\ |\ i) = P_{ik}$$
- 더 정확한 식 표현 : $$dot\ product(w_{i}\ \tilde{w_{k}}) \approx\ log\ P(k\ |\ i) = log\ P_{ik}$$

- 임베딩 벡터를 만들기 위한 손실 함수 설계하기
  - 가장 중요한 것은 단어 간의 관계를 잘 표현하는 함수여야 함
  - 이를 위해서 앞서 배운 $$P_{ik} / P_{jk}$$를 식에 사용
  - Glove의 연구진들은 벡터 $$w_{i}, w_{j}, \tilde{w_{k}}$$를 가지고 F를 수행하면 $$P_{ik} / P_{jk}$$가 나온다는 초기 식으로 전개 시작
    
  $$
  F(w_{i},\ w_{j},\ \tilde{w_{k}}) = \frac{P_{ik}}{P_{jk}}
  $$

  - 이 식을 목적에 맞게 근사시킬 수 있는 방법은 많으나 준동형($F(a+b) = F(a)F(b),\ \forall a,\ b\in \mathbb{R}$ , 지수함수 exp가 여기에 해당함)을 만족하게 만드는 방법을 사용
  - 나머지 식들은 나중에 공부하고 결국 마지막 손실 함수는
  $$
  Loss\ function = \sum_{m, n=1}^{V}\ (w_{m}^{T}\tilde{w_{n}} + b_{m} + \tilde{b_{n}} - logX_{mn})^{2}
  $$

  - 문제점 1 : $$log\ X_{ik}$$에서 $$X_{ik}$$값이 0이 될 수 있음. 그래서 $$log\ (1 + X_{ik})$$로 대체
  - 문제점 2 : 동시 등장 행렬 $$X$$에는 희소 행렬이라 0이 많고 동시 등장 빈도가 적어서 많은 값들이 작은 수치를 가지는 경우가 많음. $$X_{ik}$$가 낮을 경우 거의 도움이 안된다고 판단하여 가중치 함수 $$f(X_{ik})$$를 도입

    <img src="https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG" width="50%"> 

  - 위 그림은 가중치 함수로, 너무 X축이 작으면 상대적으로 함수값은 작도록하고 지나치게 높게 해도 최대값이 정해 놓음

  - 최종 일반화된 손실 함수는
  $$
  Loss\ function = \sum_{m, n=1}^{V}\ f(X_{mn})(w_{m}^{T}\tilde{w_{n}} + b_{m} + \tilde{b_{n}} - logX_{mn})^{2}
  $$

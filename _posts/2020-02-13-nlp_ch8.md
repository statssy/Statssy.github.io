---
layout: post
title:  "[요약정리]딥 러닝을 이용한 자연어 처리 입문(Ch08.딥 러닝 개요)"
subtitle:   "NLP"
categories: data
tags: nlp
comments: true
---
딥러닝에 대해 까먹은것도 있고 자연어 처리쪽은 처음이라 이해 쉽게 최대한 짧게 정리해보았다.  

---

[참고 사이트 : 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22882) 에 대한 글을 요약하였다. 

# Ch08. 딥 러닝(Deep Learning) 개요

## 1) 머신 러닝이란
- 이미지 인식 분야에서 특징 잡아내는것에 한계가 있었는데 이 문제를 머신 러닝이 해결

# Ch08. 딥러닝(Deep Learning) 개요

## 1) 퍼셉트론(Perceptron)
- 인공신경망으로 다수의 입력으로 부터 하나의 결과를 내보내는 알고리즘
- 단층 퍼셉트론 : 입력층과 출력층만 있는 경우(XOR문제 못품)
- 다층 퍼셉트론 : 입력층 출력층 사이에 은닉층 있음 (XOR문제 풀수있음)
<img src="https://wikidocs.net/images/page/24958/perceptron_4image.jpg" width="50%">

## 인공 신경망(Artificial Neural Network) 훑어보기
- 신경망 종류
    - 피드 포워드 신경망(Feed-Forward Neural Network, FFNN) : 입력층에서 출력층 방향으로 가는 신경망
    <img src="https://wikidocs.net/images/page/24987/mlp_final.PNG" width="50%"> 
    
    - 순환 신경망(Recurrent Neural Network, RNN) : 은닉층의 출력이 다른 은닉층으로 가는 신경망
    <img src="https://wikidocs.net/images/page/24987/rnn_final.PNG" width="50%"> 

- 전결합층(Fully-Connected Layer, FC, Dense Layer)
: 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있는 층을 전결합층이라고 한다. 밀집층이라고도 한다.

- 활성화 함수(Activation Function) : 출력값을 결정하는 함수
<img src="https://wikidocs.net/images/page/24987/activation_function_final.PNG" width="50%">

- 활성화 함수 특징 : 비선형 함수여야 한다. y = kx에서 k만 제곱한 꼴 밖에 안되니까

- 활성화 함수 종류
    - 계단 함수
    <img src="https://wikidocs.net/images/page/24987/step_function.PNG" width="50%">

    - 시그모이드 (기울기 소실 중요)
    : 역전파 과정에서 0에 가까운 기울기가 곱해지면 기울기 소실(Vanishing Gradient) 일어남
    그러므로 시그모이드 함수는 은닉층에 쓰지 않음
    <img src="https://wikidocs.net/images/page/60683/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%982.PNG" width="50%">
    <img src="https://wikidocs.net/images/page/60683/%EA%B8%B0%EC%9A%B8%EA%B8%B0_%EC%86%8C%EC%8B%A4.png" width="50%">

    - 하이퍼볼릭탄젠트 함수
    : y가 -1과 1 사이여서 반환값이 시그모이드 함수보다는 기울기 소실이 적은편
    <img src="https://wikidocs.net/images/page/60683/%ED%95%98%EC%9D%B4%ED%8D%BC%EB%B3%BC%EB%A6%AD%ED%83%84%EC%A0%A0%ED%8A%B8.PNG" width="50%">

    - 렐루 함수(ReLU)
    : 깊은 신경망에 잘 작동 / 연산이 필요없어서 속도 빠름 / 입력값이 음수면 기울기도 0이라 회생 불가(그래서 리키 렐루 씀)
    <img src="https://wikidocs.net/images/page/60683/%EB%A0%90%EB%A3%A8%ED%95%A8%EC%88%98.PNG" width="50%">

    - 리키 렐루(Leaky ReLU)
    : 입력값이 음수면 기울기도 0이라 회생 불가여서 씀
    <img src="https://wikidocs.net/images/page/60683/%EB%A6%AC%ED%82%A4%EB%A0%90%EB%A3%A8.PNG" width="50%">

    - 소프트맥스 함수(Softmax Function)
    : 은닉층에 ReLU쓰는게 일반적이지만 시그모이드나 소프트맥스 안쓰는것은 아님.
    분류 문제에서는 로지스틱 회귀와 소프트맥스 회귀를 출력층에 적용해서 사용
    <img src="https://wikidocs.net/images/page/60683/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.PNG" width="50%">

## 3) 딥러닝의 학습방법
- 순전파(Foward Propagation) : 입력층에서 출력층 방향으로 가는 과정
- 손실 함수(Loss Function) : 실제값 예측값 차이
    - MSE : 오차제곱 평균
    - 크로스 엔트로피(Cross-Entropy)
- 옵티마이저(Optimizer) : 손실 함수 줄여가면서 학습하는데 배치(가중치 조정에 사용되는 데이터량)가 중요한 개념
    - 배치 경사 하강법(Batch Gradient Descent) : 한번에 에포크에 모든 매개변수 업데이트 한번 수행(오래걸림)
    - 확률적 경사 하강법(Stochastic Gradient Descent, SGD) : 랜덤으로 적은 데이터 사용
    <img src="https://wikidocs.net/images/page/24987/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95SGD.PNG" width="50%">
    - 미니 배치 경사 하강법(Mini-Batch Gradient Descent) : 정해진 양에 대해서만 사용
    - 모멘텀(Momentum) : 공 떨어질때 관성의 힘으로 다른 언덕을 넘는 느낌
    <img src="https://wikidocs.net/images/page/24987/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG" width="50%">
    - 아다그라드(Adagrad) : 변화가 많은 매개변수는 학습률 작게하고 변화가 적은 매개변수는 학습률 높게 설정
    - 알엠에스프롭(RMSprop) : 아다그라드가 학습률이 너무 지나치게 떨어지는 거 막음
    - 아담(Adam) : 알엠에스프롭하고 모멘텀 합친듯한 방법

- 에포크와 배치 크기, 이터레이션
    - 에포크 : 순전파 갔다가 역전파 한번 간 그 상태
    - 배치 : 부분 집합안에 수
    - 이터레이션 : 1에포크 끝낼때 필요한 배치의 수 (부분 집합의 수)

<img src="https://wikidocs.net/images/page/36033/batchandepochiteration.PNG" width="50%">

- 역전파 : 옵티마이저로 가중치 업데이트

## 4) 과적합(Overfiitting)을 막는 방법
- 데이터 수 늘리기
- 모델 복잡도 줄이기
- 가중치 규제(Regularization) 적용 : λ는 규제의 강도 하이퍼파라미터로 이게 크면 규제를 위해 항들을 작게 만드는 게 우선적이라고 보면됨. (기존 비용함수+가중치규제 식이라고 보면됨)
    - L1 규제 : 가중치들의 절대값 합계를 비용 함수에 추가
    - L2 규제 : 가중치들의 제곱 합계를 비용 함수에 추가
- 드롭아웃(Dropout) : 신경망 일부를 사용하지 않는 방법. 0.5면 절반을 안쓴다는 얘기, 신경망 학습에만 쓰고 예측시에는 사용하지 않는것이 일반적

## 5) 기울기 소실(Gradient Vanishing)과 폭주(Exploding)
: 역전파에서 기울기가 작아지는 형태 혹은 너무 커서 발산되는 형태가 있을것임

- 기울기 소실 폭주 막는법
    - ReLU 사용하기
    - 그래디언트 클리핑(Gradient Clipping) : 기울기 폭주 막기위해 임계값을 정해놓음. RNN에서 유용
    - 가중치 초기화(Weight initialization) : 초기값을 무엇으로 설정하느냐에 따라 모델 훈련이 달라짐
        - 세이비어 초기화(Xavier Initialization) : 이전층 뉴런 개수와 다음층 뉴런 개수로 계산
        - He 초기화(He initialization) : 이전층 뉴런 개수만으로 계산
    - 배치 정규화(Batch Normalization)
        - 내부 공변량 변화(Internal Covariate Shift) : 층마다 입력 데이터 분포가 달라짐
        - 배치 정규화(Batch Normalization) : 배치 단위로 정규화
        - 배치 정규화의 한계 : 미니 배치 크기에 의존적(배치크기 1로하면 분산 0), RNN 적용 힘듬(시점 마다 다른 통계치를 가져서)
    - 층 정규화 : 배치 정규화는 하나의 피쳐에 대한 데이터를 정규화 한다면 층 정규화는 obs당 정규화

## 6) 케라스 훑어보기 (순서)
1. 전처리
    - 단어 토큰화
    - 패딩해서 길이 맞추기
2. 워드 임베딩 : 저차원 임베딩 벡터로 만든다. 원핫벡터는 너무 고차원이니까 
3. 모델링 : 모델 층 쌓는다. 활성화함수(ex. sigmoid)을 고른다.
4. 컴파일과 훈련
    - 컴파일 : 손실함수(ex.MSE), 최적화 방법(손실함수 줄여가는 방법 ex.rmsprop), 메트릭함수(ex.acc정확도) 선택해서 
    - 훈련 : 에포크와 배치 사이즈 정하기, 검증 데이터를 따로 할지 or 트레인데이터에서 떼어 올지 선택, verbose = 사용해서 훈련 진행바 보여줄지 등등
5. 평가(Evaluation)와 예측(Prediction)
    - 평가 : evaluate()로 정확도 평가
    - 예측 : predict()로 예측
6. 모델의 저장과 로드
    - 저장 : save()로 신경망 모델을 hdf5파일에 저장
    - 로드 : load()로 모델 불러오기
7. 함수형 API(functional API)
    - 복잡한 모델을 설계하려면 써야됨
    - 입력데이터 크기를 인자로 입력층에 정의해줘야하는게 이전 케라스 방법(sequential API)과 다른점

## 7) 다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기
- 피드 포워드 신경망이 가장 기본적인 형태

## 8) 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)
- 기존의 N-gram 언어 모델의 한계가 있었음
- 단어의 의미적 유사성 : 워드 임베딩으로 단어간 유사도 반영한 벡터를 만듬
- 수많은 문장에서의 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게됨. 그래서 훈련이 끝나면 다음 단어 예측 과정에서도 훈련 코퍼스에 없었던 단어 스퀀스라고 하더라도 다음 단어 선택 가능 
- 이점 : 밀집벡터(dense vector)를 사용해서 단어 유사도를 표현할 수 있었고, 더 이상 모든 n-gram을 저장할 필요가 없어서 저장 공간 이점을 가짐
- 한계 : NNLM도 n-gram과 마찬가지로 정해진 n개의 단어만 참고, 문장길이가 다 달라서 처리 필요
